setwd("/Users/antonio/Research/2024/CS-Metric/Replication-Package/code-summarization-metric/Results/run-on-test")
t<-read.csv("human-annotated-dataset-with-metrics.csv")
summary(t)
usermetrics<-c("Overall.DA.Score","Content.Adequacy","Conciseness","Fluency")
evalmetrics=c("jaccard","bleu.A","bleu.1","bleu.2","bleu.3","bleu.4","tfidf_cosine","USE_cosine_similarity",
"bert.score.precision","bert.score.recall","bert.score.f1","sentence_bert_cosine_similarity","infersent_cosine_similarity",
"meteor","rouge.l.f1","c_coeff",
"rouge.1.f1","rouge.1.p","rouge.1.r","rouge.2.f1","rouge.2.p","rouge.2.r","rouge.3.f1","rouge.3.p","rouge.3.r","rouge.4.f1",
"rouge.4.p","rouge.4.r","rouge.l.p","rouge.l.r","rouge.w.f1","rouge.w.p","rouge.w.r","tfidf_euclidean","chrf_score",
"USE_euclidean_distance","sentence_bert_euclidean_distance","infersent_euclidean_distance","CodeT5.plus.cosine.similarity","SIDE_HARD")
allmetrics=c(usermetrics,evalmetrics)
tsum<-subset(t,t$mid!=0)
attach(tsum,warn.conflicts=FALSE)
library(Hmisc)
v<-varclus(as.matrix(tmet,similarity="spearman",type="data.matrix"))
plot(v)
library(Hmisc)
install.packages("Hmisc")
library(Hmisc)
